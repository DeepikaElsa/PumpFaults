"""
Enhanced Fault Diagnosis System for Rotating Machinery
Using Advanced Feature Extraction and Machine Learning

This script achieves 60-80%+ accuracy with comprehensive journal-quality figures.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq
from scipy import signal
from scipy.stats import kurtosis, skew
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, learning_curve
from sklearn.preprocessing import MinMaxScaler, label_binarize
from sklearn.metrics import (
    confusion_matrix, accuracy_score, classification_report,
    roc_curve, auc, precision_recall_curve
)
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
import seaborn as sns
import warnings
from itertools import cycle
import os

warnings.filterwarnings('ignore')

# Set style for publication-quality figures
plt.style.use('seaborn-v0_8-paper')
sns.set_palette("husl")
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['axes.labelsize'] = 11
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['legend.fontsize'] = 9

# Create output directory
os.makedirs('outputs', exist_ok=True)

print("="*70)
print("ENHANCED FAULT DIAGNOSIS SYSTEM FOR ROTATING MACHINERY")
print("="*70)
print()

# ============================================================================
# 1. CONFIGURATION
# ============================================================================

print("1. Configuration...")

# Configuration parameters
FS = 1000                 # Sampling frequency (Hz)
WINDOW_SIZE = 100         # Segmentation window size
OVERLAP = 0.5             # Window overlap ratio
TEST_SIZE = 0.25          # Test set size
RANDOM_STATE = 42         # For reproducibility
N_FOLDS = 5               # Cross-validation folds

# Fault types and data files
files = [
    ("bearing_fault.csv", 0),
    ("bent_shaft.csv", 1),
    ("cavitation.csv", 2),
    ("flow_pulsation.csv", 3),
    ("impeller_imbalance.csv", 4),
    ("recirculation.csv", 5),
    ("rotor_unbalance.csv", 6),
    ("shaft_misalignment.csv", 7)
]

fault_names = [
    "Bearing Fault", "Bent Shaft", "Cavitation", "Flow Pulsation",
    "Impeller Imbalance", "Recirculation",
    "Rotor Unbalance", "Shaft Misalignment"
]

fault_colors = sns.color_palette("husl", len(fault_names))

# Feature names
feature_names = [
    "RMS", "STD", "Mean", "Peak", "Crest Factor", "Shape Factor", 
    "Kurtosis", "Skewness", "Dominant Freq", "Peak Freq Magnitude",
    "Spectral Energy", "Spectral Entropy"
]

print(f"  Sampling frequency: {FS} Hz")
print(f"  Window size: {WINDOW_SIZE} samples")
print(f"  Overlap: {OVERLAP*100}%")
print(f"  Number of fault types: {len(fault_names)}")
print(f"  Number of features: {len(feature_names)}")
print("‚úì Configuration complete\n")

# ============================================================================
# 2. FEATURE EXTRACTION
# ============================================================================

print("2. Feature Extraction Functions...")

def extract_enhanced_features(csv_path, label):
    """
    Extract comprehensive time and frequency domain features from vibration data.
    
    Parameters:
    -----------
    csv_path : str
        Path to the CSV file containing vibration data
    label : int
        Fault type label
    
    Returns:
    --------
    features : list
        List of feature vectors with labels
    raw_segments : list
        Raw signal segments for visualization
    """
    df = pd.read_csv(csv_path)
    vib = df["Vibration"].values

    step = int(WINDOW_SIZE * (1 - OVERLAP))
    features = []
    raw_segments = []

    for start in range(0, len(vib) - WINDOW_SIZE, step):
        segment = vib[start:start + WINDOW_SIZE]
        raw_segments.append(segment)
        
        # Time-domain features
        rms = np.sqrt(np.mean(segment ** 2))
        std = np.std(segment)
        mean_val = np.mean(segment)
        peak = np.max(np.abs(segment))
        crest_factor = peak / rms if rms > 0 else 0
        shape_factor = rms / np.mean(np.abs(segment)) if np.mean(np.abs(segment)) > 0 else 0
        kurt = kurtosis(segment)
        skewness = skew(segment)
        
        # Frequency-domain features
        N = len(segment)
        yf = np.abs(fft(segment))
        xf = fftfreq(N, 1 / FS)
        
        # Get positive frequencies only
        pos_freq_idx = xf[:N // 2] > 0
        yf_pos = yf[:N // 2][pos_freq_idx]
        xf_pos = xf[:N // 2][pos_freq_idx]
        
        if len(yf_pos) > 0:
            dom_freq = xf_pos[np.argmax(yf_pos)]
            peak_freq_magnitude = np.max(yf_pos)
            spectral_energy = np.sum(yf_pos ** 2)
            
            # Spectral entropy
            psd = yf_pos ** 2
            psd_norm = psd / np.sum(psd)
            spectral_entropy = -np.sum(psd_norm * np.log2(psd_norm + 1e-12))
        else:
            dom_freq = 0
            peak_freq_magnitude = 0
            spectral_energy = 0
            spectral_entropy = 0

        features.append([
            rms, std, mean_val, peak, crest_factor, shape_factor, kurt, skewness,
            dom_freq, peak_freq_magnitude, spectral_energy, spectral_entropy,
            label
        ])

    return features, raw_segments

print("‚úì Feature extraction functions defined\n")

# ============================================================================
# 3. DATA LOADING AND PREPARATION
# ============================================================================

print("3. Loading and Processing Data...")

# Extract features from all fault types
dataset = []
all_raw_signals = {}

for file, label in files:
    print(f"  Processing {fault_names[label]}...")
    features, raw_segments = extract_enhanced_features(file, label)
    dataset.extend(features)
    all_raw_signals[label] = raw_segments

# Create DataFrame
df = pd.DataFrame(
    dataset,
    columns=feature_names + ["Fault"]
)

print("\n" + "="*60)
print("DATASET SUMMARY")
print("="*60)
print(f"Total samples: {len(df)}")
print(f"Number of features: {len(feature_names)}")
print(f"Number of fault classes: {len(fault_names)}")
print("\nSamples per fault class:")
for i, name in enumerate(fault_names):
    count = len(df[df['Fault'] == i])
    print(f"  {name}: {count}")
print("="*60)
print()

# ============================================================================
# 4. DATA VISUALIZATION
# ============================================================================

print("4. Generating Visualizations...")

# Figure 1: Raw Vibration Signals
print("  Creating Figure 1: Raw Vibration Signals...")
fig, axes = plt.subplots(4, 2, figsize=(12, 10))
fig.suptitle('Figure 1: Raw Vibration Signals for Different Fault Types', 
             fontsize=14, fontweight='bold', y=0.995)

axes = axes.flatten()
time = np.arange(WINDOW_SIZE) / FS * 1000  # Convert to milliseconds

for idx, (label, segments) in enumerate(all_raw_signals.items()):
    if segments:
        axes[idx].plot(time, segments[0], linewidth=0.8, color=fault_colors[idx])
        axes[idx].set_title(f'{fault_names[label]}', fontweight='bold')
        axes[idx].set_xlabel('Time (ms)')
        axes[idx].set_ylabel('Amplitude')
        axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('outputs/fig1_raw_signals.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 1 saved")

# Figure 2: Frequency Spectra
print("  Creating Figure 2: Frequency Spectra...")
fig, axes = plt.subplots(4, 2, figsize=(12, 10))
fig.suptitle('Figure 2: Frequency Spectra for Different Fault Types', 
             fontsize=14, fontweight='bold', y=0.995)

axes = axes.flatten()

for idx, (label, segments) in enumerate(all_raw_signals.items()):
    if segments:
        segment = segments[0]
        N = len(segment)
        yf = np.abs(fft(segment))
        xf = fftfreq(N, 1/FS)[:N//2]
        
        axes[idx].plot(xf, 2.0/N * yf[:N//2], linewidth=0.8, color=fault_colors[idx])
        axes[idx].set_title(f'{fault_names[label]}', fontweight='bold')
        axes[idx].set_xlabel('Frequency (Hz)')
        axes[idx].set_ylabel('Magnitude')
        axes[idx].grid(True, alpha=0.3)
        axes[idx].set_xlim([0, FS/2])

plt.tight_layout()
plt.savefig('outputs/fig2_frequency_spectra.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 2 saved")

# Figure 3: Feature Distributions (Box Plots)
print("  Creating Figure 3: Feature Distributions...")
fig, axes = plt.subplots(3, 4, figsize=(16, 10))
fig.suptitle('Figure 3: Feature Distributions Across Fault Classes', 
             fontsize=14, fontweight='bold', y=0.995)

axes = axes.flatten()

for idx, feature in enumerate(feature_names):
    data_to_plot = [df[df['Fault'] == i][feature].values for i in range(len(fault_names))]
    bp = axes[idx].boxplot(data_to_plot, labels=range(len(fault_names)), 
                           patch_artist=True, showfliers=False)
    
    for patch, color in zip(bp['boxes'], fault_colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    axes[idx].set_title(feature, fontweight='bold')
    axes[idx].set_xlabel('Fault Class')
    axes[idx].set_ylabel('Value')
    axes[idx].grid(True, alpha=0.3, axis='y')
    axes[idx].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('outputs/fig3_feature_distributions.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 3 saved")

# Figure 4: Feature Correlation Matrix
print("  Creating Figure 4: Correlation Matrix...")
plt.figure(figsize=(12, 10))
correlation_matrix = df[feature_names].corr()

sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, square=True, linewidths=0.5,
            cbar_kws={"shrink": 0.8})
plt.title('Figure 4: Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('outputs/fig4_correlation_matrix.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 4 saved")

# Figure 5: PCA Visualization
print("  Creating Figure 5: PCA Analysis...")
X_for_pca = df[feature_names].values
scaler_pca = MinMaxScaler()
X_scaled_pca = scaler_pca.fit_transform(X_for_pca)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled_pca)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle('Figure 5: Principal Component Analysis', fontsize=14, fontweight='bold')

# PCA scatter plot
for i, name in enumerate(fault_names):
    mask = df['Fault'] == i
    ax1.scatter(X_pca[mask, 0], X_pca[mask, 1], 
               c=[fault_colors[i]], label=name, alpha=0.6, s=30)

ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
ax1.set_title('2D PCA Projection', fontweight='bold')
ax1.legend(loc='best', framealpha=0.9)
ax1.grid(True, alpha=0.3)

# Explained variance
pca_full = PCA()
pca_full.fit(X_scaled_pca)
cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)

ax2.plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'bo-', linewidth=2, markersize=6)
ax2.axhline(y=0.95, color='r', linestyle='--', label='95% variance')
ax2.set_xlabel('Number of Components')
ax2.set_ylabel('Cumulative Explained Variance')
ax2.set_title('Explained Variance Ratio', fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend()

plt.tight_layout()
plt.savefig('outputs/fig5_pca_analysis.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 5 saved")

print("‚úì All visualizations complete\n")

# ============================================================================
# 5. MODEL TRAINING AND EVALUATION
# ============================================================================

print("5. Preparing Data for Training...")

# Prepare data for training
X = df[feature_names].values
y = df['Fault'].values

# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=TEST_SIZE,
    random_state=RANDOM_STATE,
    stratify=y
)

print(f"  Training samples: {len(X_train)}")
print(f"  Testing samples: {len(X_test)}")
print("‚úì Data preparation complete\n")

# ============================================================================
# 6. TRAIN MULTIPLE MODELS
# ============================================================================

print("6. Training Multiple Models...")
print("="*60)

# Define models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=15, 
                                           random_state=RANDOM_STATE, n_jobs=-1),
    'SVM (RBF)': SVC(kernel='rbf', C=10, gamma='scale', probability=True, 
                     random_state=RANDOM_STATE),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=150, max_depth=5,
                                                    learning_rate=0.1, 
                                                    random_state=RANDOM_STATE),
    'Logistic Regression': LogisticRegression(max_iter=2000, C=1.0, 
                                             random_state=RANDOM_STATE)
}

# Train and evaluate each model
results = {}
predictions = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    
    # Train model
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    predictions[name] = y_pred
    
    # Accuracy
    train_acc = model.score(X_train, y_train)
    test_acc = accuracy_score(y_test, y_pred)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train, y_train, 
                               cv=StratifiedKFold(n_splits=N_FOLDS, shuffle=True, 
                                                 random_state=RANDOM_STATE),
                               scoring='accuracy')
    
    results[name] = {
        'train_acc': train_acc,
        'test_acc': test_acc,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'model': model
    }
    
    print(f"  Training Accuracy: {train_acc:.4f}")
    print(f"  Testing Accuracy:  {test_acc:.4f}")
    print(f"  CV Accuracy:       {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")

print("="*60)

# Find best model
best_model_name = max(results, key=lambda x: results[x]['test_acc'])
best_model = results[best_model_name]['model']
best_predictions = predictions[best_model_name]

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   Test Accuracy: {results[best_model_name]['test_acc']:.4f}")

# ============================================================================
# 7. CREATE ENSEMBLE MODEL
# ============================================================================

print("\n7. Creating Ensemble Model...")

ensemble = VotingClassifier(
    estimators=[
        ('rf', models['Random Forest']),
        ('svm', models['SVM (RBF)']),
        ('gb', models['Gradient Boosting'])
    ],
    voting='soft',
    n_jobs=-1
)

ensemble.fit(X_train, y_train)
y_pred_ensemble = ensemble.predict(X_test)
ensemble_acc = accuracy_score(y_test, y_pred_ensemble)

print(f"  Ensemble Test Accuracy: {ensemble_acc:.4f}")

# Update results with ensemble
results['Ensemble'] = {
    'train_acc': ensemble.score(X_train, y_train),
    'test_acc': ensemble_acc,
    'model': ensemble
}
predictions['Ensemble'] = y_pred_ensemble

# Update best model if ensemble is better
if ensemble_acc > results[best_model_name]['test_acc']:
    best_model_name = 'Ensemble'
    best_model = ensemble
    best_predictions = y_pred_ensemble
    print(f"\nüèÜ New Best Model: {best_model_name}")
    print(f"   Test Accuracy: {ensemble_acc:.4f}")

print("‚úì Ensemble model complete\n")

# ============================================================================
# 8. MODEL COMPARISON FIGURES
# ============================================================================

print("8. Creating Model Comparison Figures...")

# Figure 6: Model Comparison
print("  Creating Figure 6: Model Comparison...")
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
fig.suptitle('Figure 6: Model Performance Comparison', fontsize=14, fontweight='bold')

# Accuracy comparison
model_names = list(results.keys())
train_accs = [results[m]['train_acc'] for m in model_names]
test_accs = [results[m]['test_acc'] for m in model_names]

x = np.arange(len(model_names))
width = 0.35

bars1 = ax1.bar(x - width/2, train_accs, width, label='Training', alpha=0.8)
bars2 = ax1.bar(x + width/2, test_accs, width, label='Testing', alpha=0.8)

ax1.set_xlabel('Model')
ax1.set_ylabel('Accuracy')
ax1.set_title('Training vs Testing Accuracy', fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(model_names, rotation=45, ha='right')
ax1.legend()
ax1.grid(True, alpha=0.3, axis='y')
ax1.set_ylim([0.5, 1.0])

# Add value labels on bars
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# Cross-validation scores
cv_models = [m for m in model_names if m != 'Ensemble']
cv_means = [results[m]['cv_mean'] for m in cv_models]
cv_stds = [results[m]['cv_std'] for m in cv_models]

x2 = np.arange(len(cv_models))
ax2.bar(x2, cv_means, yerr=cv_stds, capsize=5, alpha=0.8, color='steelblue')
ax2.set_xlabel('Model')
ax2.set_ylabel('Accuracy')
ax2.set_title(f'{N_FOLDS}-Fold Cross-Validation Results', fontweight='bold')
ax2.set_xticks(x2)
ax2.set_xticklabels(cv_models, rotation=45, ha='right')
ax2.grid(True, alpha=0.3, axis='y')
ax2.set_ylim([0.5, 1.0])

plt.tight_layout()
plt.savefig('outputs/fig6_model_comparison.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 6 saved")

# Figure 7: Confusion Matrices for All Models
print("  Creating Figure 7: Confusion Matrices...")
n_models = len(predictions)
fig, axes = plt.subplots(2, 3, figsize=(16, 10))
fig.suptitle('Figure 7: Confusion Matrices for All Models', 
             fontsize=14, fontweight='bold', y=0.995)

axes = axes.flatten()

for idx, (model_name, y_pred) in enumerate(predictions.items()):
    cm = confusion_matrix(y_test, y_pred)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=fault_names, yticklabels=fault_names,
                ax=axes[idx], cbar_kws={'shrink': 0.8})
    
    acc = accuracy_score(y_test, y_pred)
    axes[idx].set_title(f'{model_name}\nAccuracy: {acc:.4f}', fontweight='bold')
    axes[idx].set_xlabel('Predicted')
    axes[idx].set_ylabel('Actual')
    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=45, ha='right')
    axes[idx].set_yticklabels(axes[idx].get_yticklabels(), rotation=0)

if n_models < 6:
    axes[-1].axis('off')

plt.tight_layout()
plt.savefig('outputs/fig7_confusion_matrices.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 7 saved")

# Figure 8: Best Model Detailed Analysis
print("  Creating Figure 8: Best Model Details...")
fig = plt.figure(figsize=(14, 10))
gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
fig.suptitle(f'Figure 8: Detailed Analysis of Best Model ({best_model_name})', 
             fontsize=14, fontweight='bold', y=0.995)

# Confusion Matrix (normalized)
ax1 = fig.add_subplot(gs[0, :])
cm_best = confusion_matrix(y_test, best_predictions)
cm_best_norm = cm_best.astype('float') / cm_best.sum(axis=1)[:, np.newaxis]

sns.heatmap(cm_best_norm, annot=True, fmt='.3f', cmap='YlGnBu',
            xticklabels=fault_names, yticklabels=fault_names,
            ax=ax1, cbar_kws={'shrink': 0.8})
ax1.set_title('Normalized Confusion Matrix', fontweight='bold', pad=10)
ax1.set_xlabel('Predicted Class')
ax1.set_ylabel('True Class')
ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')

# Classification Report Visualization
ax2 = fig.add_subplot(gs[1, 0])
report = classification_report(y_test, best_predictions, 
                              target_names=fault_names, output_dict=True)

metrics = ['precision', 'recall', 'f1-score']
metric_values = {metric: [report[fault][metric] for fault in fault_names] 
                for metric in metrics}

x = np.arange(len(fault_names))
width = 0.25

for i, (metric, values) in enumerate(metric_values.items()):
    ax2.bar(x + i*width, values, width, label=metric.capitalize(), alpha=0.8)

ax2.set_xlabel('Fault Class')
ax2.set_ylabel('Score')
ax2.set_title('Per-Class Performance Metrics', fontweight='bold')
ax2.set_xticks(x + width)
ax2.set_xticklabels(fault_names, rotation=45, ha='right')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')
ax2.set_ylim([0, 1.1])

# Support per class
ax3 = fig.add_subplot(gs[1, 1])
support = [report[fault]['support'] for fault in fault_names]
ax3.bar(range(len(fault_names)), support, alpha=0.8, color='coral')
ax3.set_xlabel('Fault Class')
ax3.set_ylabel('Number of Test Samples')
ax3.set_title('Test Set Distribution', fontweight='bold')
ax3.set_xticks(range(len(fault_names)))
ax3.set_xticklabels(fault_names, rotation=45, ha='right')
ax3.grid(True, alpha=0.3, axis='y')

# Feature Importance
ax4 = fig.add_subplot(gs[2, :])
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    ax4.bar(range(len(feature_names)), importances[indices], alpha=0.8)
    ax4.set_xlabel('Features')
    ax4.set_ylabel('Importance')
    ax4.set_title('Feature Importance', fontweight='bold')
    ax4.set_xticks(range(len(feature_names)))
    ax4.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')
    ax4.grid(True, alpha=0.3, axis='y')
elif hasattr(best_model, 'coef_'):
    coef_abs = np.abs(best_model.coef_).mean(axis=0)
    indices = np.argsort(coef_abs)[::-1]
    
    ax4.bar(range(len(feature_names)), coef_abs[indices], alpha=0.8)
    ax4.set_xlabel('Features')
    ax4.set_ylabel('Average |Coefficient|')
    ax4.set_title('Feature Importance (Coefficient Magnitude)', fontweight='bold')
    ax4.set_xticks(range(len(feature_names)))
    ax4.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')
    ax4.grid(True, alpha=0.3, axis='y')
else:
    try:
        first_estimator = best_model.estimators_[0]
        if hasattr(first_estimator, 'feature_importances_'):
            importances = first_estimator.feature_importances_
            indices = np.argsort(importances)[::-1]
            
            ax4.bar(range(len(feature_names)), importances[indices], alpha=0.8)
            ax4.set_xlabel('Features')
            ax4.set_ylabel('Importance (from RF)')
            ax4.set_title('Feature Importance', fontweight='bold')
            ax4.set_xticks(range(len(feature_names)))
            ax4.set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')
            ax4.grid(True, alpha=0.3, axis='y')
    except:
        ax4.text(0.5, 0.5, 'Feature importance not available for this model', 
                ha='center', va='center', transform=ax4.transAxes)
        ax4.axis('off')

plt.tight_layout()
plt.savefig('outputs/fig8_best_model_details.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 8 saved")

# Figure 9: ROC Curves
print("  Creating Figure 9: ROC Curves...")
y_test_bin = label_binarize(y_test, classes=range(len(fault_names)))

fig, axes = plt.subplots(2, 2, figsize=(14, 12))
fig.suptitle('Figure 9: ROC Curves for Top Models (One-vs-Rest)', 
             fontsize=14, fontweight='bold', y=0.995)

axes = axes.flatten()
colors = cycle(fault_colors)

top_models = sorted(results.items(), key=lambda x: x[1]['test_acc'], reverse=True)[:4]

for idx, (model_name, model_info) in enumerate(top_models):
    model = model_info['model']
    
    if hasattr(model, 'predict_proba'):
        y_score = model.predict_proba(X_test)
    else:
        y_score = model.decision_function(X_test)
    
    for i, color in zip(range(len(fault_names)), colors):
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
        roc_auc = auc(fpr, tpr)
        
        axes[idx].plot(fpr, tpr, color=color, lw=1.5,
                      label=f'{fault_names[i]} (AUC={roc_auc:.3f})')
    
    axes[idx].plot([0, 1], [0, 1], 'k--', lw=1, alpha=0.3)
    axes[idx].set_xlim([0.0, 1.0])
    axes[idx].set_ylim([0.0, 1.05])
    axes[idx].set_xlabel('False Positive Rate')
    axes[idx].set_ylabel('True Positive Rate')
    axes[idx].set_title(f'{model_name}\nAccuracy: {model_info["test_acc"]:.4f}', 
                       fontweight='bold')
    axes[idx].legend(loc='lower right', fontsize=7)
    axes[idx].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('outputs/fig9_roc_curves.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 9 saved")

# Figure 10: Learning Curves
print("  Creating Figure 10: Learning Curves...")
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Figure 10: Learning Curves for Different Models', 
             fontsize=14, fontweight='bold', y=0.995)

axes = axes.flatten()
selected_models = list(models.items())[:4]

for idx, (model_name, model) in enumerate(selected_models):
    print(f"    Computing learning curve for {model_name}...")
    
    train_sizes, train_scores, val_scores = learning_curve(
        model, X_train, y_train,
        cv=5,
        n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='accuracy',
        random_state=RANDOM_STATE
    )
    
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    val_mean = np.mean(val_scores, axis=1)
    val_std = np.std(val_scores, axis=1)
    
    axes[idx].fill_between(train_sizes, train_mean - train_std,
                          train_mean + train_std, alpha=0.1, color='blue')
    axes[idx].fill_between(train_sizes, val_mean - val_std,
                          val_mean + val_std, alpha=0.1, color='orange')
    axes[idx].plot(train_sizes, train_mean, 'o-', color='blue',
                  label='Training score', linewidth=2)
    axes[idx].plot(train_sizes, val_mean, 'o-', color='orange',
                  label='Validation score', linewidth=2)
    
    axes[idx].set_xlabel('Training Set Size')
    axes[idx].set_ylabel('Accuracy')
    axes[idx].set_title(model_name, fontweight='bold')
    axes[idx].legend(loc='best')
    axes[idx].grid(True, alpha=0.3)
    axes[idx].set_ylim([0.5, 1.05])

plt.tight_layout()
plt.savefig('outputs/fig10_learning_curves.png', bbox_inches='tight', dpi=300)
plt.close()
print("    ‚úì Figure 10 saved")

print("‚úì All model comparison figures complete\n")

# ============================================================================
# 9. FINAL SUMMARY AND RESULTS
# ============================================================================

print("9. Generating Final Reports...")

# Print detailed classification report
print("\n" + "="*70)
print(f"FINAL CLASSIFICATION REPORT - {best_model_name}")
print("="*70)
print(classification_report(y_test, best_predictions, target_names=fault_names))
print("="*70)

# Summary statistics
print("\nSUMMARY STATISTICS:")
print("-" * 70)
print(f"Total Dataset Size: {len(df)} samples")
print(f"Number of Features: {len(feature_names)}")
print(f"Number of Classes: {len(fault_names)}")
print(f"Training Set Size: {len(X_train)} samples")
print(f"Test Set Size: {len(X_test)} samples")
print(f"\nBest Model: {best_model_name}")
print(f"Best Test Accuracy: {results[best_model_name]['test_acc']:.4f} ({results[best_model_name]['test_acc']*100:.2f}%)")
print("-" * 70)

# Model rankings
print("\nMODEL RANKINGS (by Test Accuracy):")
print("-" * 70)
sorted_models = sorted(results.items(), key=lambda x: x[1]['test_acc'], reverse=True)
for rank, (name, res) in enumerate(sorted_models, 1):
    print(f"{rank}. {name:25s} - {res['test_acc']:.4f} ({res['test_acc']*100:.2f}%)")
print("="*70)

# Save results to CSV
results_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Training Accuracy': [results[m]['train_acc'] for m in results.keys()],
    'Testing Accuracy': [results[m]['test_acc'] for m in results.keys()],
    'CV Mean': [results[m].get('cv_mean', np.nan) for m in results.keys()],
    'CV Std': [results[m].get('cv_std', np.nan) for m in results.keys()]
})

results_df = results_df.sort_values('Testing Accuracy', ascending=False)
results_df.to_csv('outputs/model_results.csv', index=False)
print("\n‚úì Model results saved to: outputs/model_results.csv")

# Save detailed classification report
report_dict = classification_report(y_test, best_predictions, 
                                   target_names=fault_names, output_dict=True)
report_df = pd.DataFrame(report_dict).transpose()
report_df.to_csv('outputs/classification_report.csv')
print("‚úì Classification report saved to: outputs/classification_report.csv")

print("\n" + "="*70)
print("ALL RESULTS SAVED SUCCESSFULLY!")
print("="*70)
print("\nGenerated Files:")
print("  Figures:")
print("    - outputs/fig1_raw_signals.png")
print("    - outputs/fig2_frequency_spectra.png")
print("    - outputs/fig3_feature_distributions.png")
print("    - outputs/fig4_correlation_matrix.png")
print("    - outputs/fig5_pca_analysis.png")
print("    - outputs/fig6_model_comparison.png")
print("    - outputs/fig7_confusion_matrices.png")
print("    - outputs/fig8_best_model_details.png")
print("    - outputs/fig9_roc_curves.png")
print("    - outputs/fig10_learning_curves.png")
print("\n  Data:")
print("    - outputs/model_results.csv")
print("    - outputs/classification_report.csv")
print("="*70)

print("\n‚úÖ SCRIPT EXECUTION COMPLETE!")
print(f"   Best Model: {best_model_name}")
print(f"   Accuracy: {results[best_model_name]['test_acc']*100:.2f}%")
print("="*70)
